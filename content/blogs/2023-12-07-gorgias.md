---
title: ""
description: ""
date: 2023-12-07T09:00:00
category: Solutions
author:
  name: Benoit Pimpaud
  image: "bpimpaud"
image: /blogs/2023-12-97-gorgias.jpg
---

Infrastructure as Code (IaC) has become the industry standard for cloud resources management and configuration.

It allows teams to easily collaborate across multiple resources through providers and have a common declarative syntax for several purposes : IAM policies, Kubernetes clusters, database users and tables etc.

A wide variety of objects can be seamlessly setup using a single language like HCL with defined dependency graphs and modularity capabilities brought by Terraform.

What if this same principle were applied to Data Engineering?

With the rise of the Modern Data Stack, workflows span across diverse tools — ranging from ETL scripts, data transformations, and databases to reverse-ETL processes and ML models.

The current state of solutions involves scattering configurations across various domains:

* Infrastructure : service account roles, datasets, Kubernetes clusters
* Data Sources (e.g., Segment, SaaS webhooks): Subscribed events, destinations
* ETL tools (Airbyte, Fivetran, custom solutions): Sources, destinations, sync frequency, retry policies
* Transformation (dbt, sqlmesh, scheduled queries) : commands, scheduling, retry policies, monitoring & alerting rules
* Reverse-ETL (Hightouch, Census, custom code): Sync schedules, destination configuration

As the number of workflows and data volume increase, Gorgias engineering teams tend to be spinning plates around workflows errors having to maintain complex logic across multiple systems that are heterogenous.

Embracing framework helps them having a more consistent stack : i.e. using Airbyte for data ingestion enforces writing connectors with a common structure. But it was not enough to keep data flow logic versioned and maintainable.

Typical data teams easily end up having 100s of sources, 1 000s of models and 1 000s workflows (Zaps, reverse-ETL syncs, custom code workflows).

Engineering and technical debts start to rise and it is more and more difficult to delegate part of the upstream scope (ingestion, transformation) to Data consumers (Data Analysts, Marketing, HR, Growth teams) as domains are mixed.

In this blog post, we'll dive into how Gorgias employ IaC principles for their data infrastructure using best-of-breed tools, including Kestra, Airbyte, dbt, Hightouch and Terraform.

## Data orchestration tool : the master piece

While internal data analysis often doesn’t necessitate real-time processing, batch processing is optimally managed through orchestration tools, offering the capability to trigger and schedule tasks, much like a cron job.

Yet, some are tempted to consolidate all logic within highly customizable systems like Airflow. Doing ETL, transformation and reverse ETL all with custom code some drawbacks.

* Orchestration logic involving complex decorators can present a steep learning curve

* Intricacies of modularity and deployment strategies, such as building Docker images and creating PyPI packages, add complexity to an already sophisticated process

* Impact on the entire stack is not always straightforward when implementing changes, especially when updating shared components

> This additional complexity limits the enablement capabilities of a stack, counter to the goal of empowering users to easily implement their workflows. As System people , the emphasis should be on encouraging self-serve capabilities while maintaining robust PR review practices.

Several months ago, Gorgias faced the limitations of embedded schedulers and embarked on a quest for a dedicated data orchestration tool.

Although many modern data tools integrate schedulers (Hightouch, Dbt cloud, Airbyte, GCP with Cloud Scheduler and scheduled queries), they often fall short in handling complex orchestration needs, such as retry policies, event-triggered workflows, and intricate dependencies.

Also, managing numerous execution logs across different tabs further underscored the need for a centralized orchestrator — a main control plane to streamline operations and enhance visibility.

## Benchmarking existing solutions for our use-cases

Gorgias engineering team look at several existing solutions that could fit their usecases. Especially Airflow and Dagster.
However both show some issues that could not answer properly their needs:

**Airflow**

* Hard to deploy: while managed versions like Cloud Composer on GCP are here to help, they are quite expensive and not easily scalable.
* Python Code: some of our team members were already quite familiar with Airflow, but their experience in maintaining a complete codebase and corresponding dependencies with it was far from ideal. While getting started with Python code is relatively easy, scaling it up requires a significant investment of effort and time.
* Poor UI: Airflow’s user interface has noticeably aged when compared to similar tools. The DAG view can be challenging for newcomers to navigate, and the overall user experience leaves much to be desired.
* Steep Learning Curve: Airflow encompasses a variety of concepts, and handling them in Python can occasionally be either overly complex or excessively wordy. Also, managing an Airflow installation on the long term can be very time consuming, both in maintenance and being up to date with the latest versions or concepts.

**Dagster**

* Painful User Code Deployment (Open Source edition): while supporting different strategies for deployments, none of them were straightforward and needed to handle code locations deployment whereas in our case we just wanted a single codebase.
* Python Code: while Python is easy to understand, having to deal with many decorators functions is a bit too much from a framework that we initially anticipated to be simple and straightforward to maintain. Our past experiences have taught us that keeping custom Python wrappers in good shape can become a daunting and arduous task over time…
* Steep learning curve: Dagster have a lot of different concepts and managing those ones in Python is sometimes too complicated or too verbose. On a long term strategy, we had doubts regarding the integration of a less technical audience into such a framework.

## Declarative Data Orchestration with Kestra

Kestra offers an innovative way to deal with data pipelines based on YAML definition. Fulfilling their need for Infrastructure As Code pattern.

Other important aspects :

* Native integration with all the tools Gorgias is already using: Airbyte, BigQuery, PostgreSQL databases, dbt (cli and cloud), GitHub, Slack, Hightouch, etc.
* Rich user interface to define tasks and manage flow executions
* Natively support Webhook trigger
* Terraform provider to allow versioning and modularity on top of Flows and thus introduces best software-engineering practices.
* Designed to scale and handle millions of flows in parallel, scaling it is made easy out of the box.
* Full control on data (self hosted on Kubernetes)
* Easy to contribute on plugins.

With Kestra, Gorgias team is addressing the following needs seamlessly :

* Triggering Airbyte sync followed by dbt transformations
* Have proper retry policies. Especially for dbt jobs, as they moved to capacity based BigQuery pricing they can end up with resource exhaustion issues more often.
* Scheduling Hightouch sync at the end of dbt jobs for better freshness
* Trigger ETL python scripts with easy monitoring based on Webhook events
* Have a centralized place for all their logs


Integrated UI allows you to define on-the-fly flows using a nice workflow editor and provide a large collection of blueprints :

IMAGE - The workflow editor allows to define tasks fully using Kestra UI

IMAGE - UI provides details for each property available

IMAGE - Blueprints are often good ways to start


## Modularity with Kestra and Terraform
In order to provide modular development experience, we can leverage Terraform modules and Kestra subflow patterns.

Modules are used as an abstraction layer for regular users to write flows (i.e. triggering an Airbyte sync followed by a dbt transformation) without having to worry about Kestra syntax, authentication or connection details.

Subflows are used for generic tasks :

They contain direct YAML and are declared within subflows/main.tf and define Inputs and Outputs clearly.
Subflows can depend on each other, this dependency should be materialized by depends_on field in subflows/main.tf.
Can be used by modules to hide some non-necessary complexity and to dry redundant tasks (Running a SQL query on a given PostgresDB)

Flow definition to trigger an Airbyte sync and launch dbt command after completion
Logs are fully stream from Airbyte to Kestra to provide a centralized platform for monitoring and debugging :


Flow execution logs

## Applying configuration using Kestra as a CI/CD tool
We talked earlier about user code deployment, let’s talk about how we can deploy flows with Terraform.

Maintaining a direct connection between an external CI / CD tools like GitHub Action to the Kestra server service involves solutions like :

Creating an ingress exposing the service : we would have to secure it using a VPN or an authentication mechanism
Using port-forward during execution : adds complexity and is not very standard
These are not ideal and would add complexity & configuration to our CI / CD. In order to be more efficient, let’s think the other way around : using Kestra as a CI / CD

As we are applying configuration change on a self-hosted instance , it’s easier to be directly on our Kubernetes cluster to run Terraform.

Kestra released a dedicated Terraform plugin which allows you to apply your changes using local k8s service endpoint using kube-dns resolution.

This way, all is handled by Kestra and Terraform Plan is easily readable in execution logs.


Terraform plan output in execution logs
You can, of course, create a python script within your Kestra flow to add it as a comment on your PRs for easier reviews :)


Topology of the “Terraform plan” flow

## Conclusion
Declarative Data Engineering is rising as Data teams embrace SWE practices and Kestra got this point right allowing data practitioners to collaborate around a common tool.

I strongly encourage you to try Kestra, it might change the way you think about data pipeline declaration and collaboration :)







Join the [Slack community](https://kestra.io/slack) if you have any questions or need assistance. Follow us on [Twitter](https://twitter.com/kestra_io) for the latest news. Check the code in our [GitHub repository](https://github.com/kestra-io/kestra) and give us a star if you like the project.
